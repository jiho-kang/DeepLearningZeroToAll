{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3a718e",
   "metadata": {},
   "source": [
    "# 모두를 위한 딥러닝 tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabf611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hello Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f79c549d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello,Tensorflow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant(\"Hello,Tensorflow!\")\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12fb86",
   "metadata": {},
   "source": [
    "# 01) Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fa7c4",
   "metadata": {},
   "source": [
    "(1) Build graph(tensors) using TensorFlow operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5916b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32) #넣을내용,데이터형식\n",
    "node2 = tf.constant(4.0) #also tf.float32 implicitly\n",
    "node3 = tf.add(node1,node2) #node3 = node1 + node2도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b4e29d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_3:0\", shape=(), dtype=float32) node2: Tensor(\"Const_3:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1:\", node2, \"node2:\", node2)\n",
    "print(\"node3:\", node3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de95278",
   "metadata": {},
   "source": [
    "(2) feed data and run graph (operation)\n",
    "    sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e5ed6",
   "metadata": {},
   "source": [
    "(3) update variables in the graph\n",
    "    (and return values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e474e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run([node1,node2]):  [3.0, 4.0]\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"sess.run([node1,node2]): \",sess.run([node1,node2]))\n",
    "print(\"sess.run(node3): \",sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348df16d",
   "metadata": {},
   "source": [
    "### Placeholder (like input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c46927",
   "metadata": {},
   "source": [
    "placeholder로 미지수로 남기고, feed_dict로 넣어줌. n개 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c100ac71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a+b #tf.add(a,b)도 가능\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a:3,b:4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28fddf",
   "metadata": {},
   "source": [
    "### Tensor Ranks, Shpaes and Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6dbc4a",
   "metadata": {},
   "source": [
    "Rank / Mateh entity / Python example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384448f6",
   "metadata": {},
   "source": [
    "    0 / Scalar(magnitude only) / s = 483\n",
    "    1 / Vector(magnitude and directioni) / v = [1.2, 3.3, 4.3]\n",
    "    2 / Matrix(table of numbers) / m = [[1,2], [5,6], [7,3]]\n",
    "    3 / 3-Tensor(cube of numbers) t = [[[2],[3],[4]],[[5],[6],[7]],[[8],[9],[10]]]\n",
    "    n / n-Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7009821",
   "metadata": {},
   "source": [
    "# 02) Linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd8f2b",
   "metadata": {},
   "source": [
    "### (1) Build graph using TF operations\n",
    "H(x) = Wx+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b016507",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "#tt.Variable = TF가 학습하는 과정에서 변경시키는 v이다.\n",
    "w = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#our hypothesis Wx+b\n",
    "hypothesis = w*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04a452",
   "metadata": {},
   "source": [
    "cost(W,b) = 1/m (시그마 i=1~m : (H(xi)-yi)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb68297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reduce_mean: 평균 값을 구해줌\n",
    "#tf.square: 제곱\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14097d66",
   "metadata": {},
   "source": [
    "GradientDescent (경사하강법)로 cost 최소값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a800ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26cb86",
   "metadata": {},
   "source": [
    "### (2),(3) Run/Update graph and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0c0473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 None 15.762543 [-1.9815823] [2.8269267]\n",
      "100 None 1.0607485 [-0.19620547] [2.719229]\n",
      "200 None 0.6523307 [0.06194156] [2.132427]\n",
      "300 None 0.401165 [0.26437336] [1.6722523]\n",
      "400 None 0.24670522 [0.4231205] [1.3113828]\n",
      "500 None 0.15171678 [0.5476103] [1.0283883]\n",
      "600 None 0.093301594 [0.6452353] [0.8064636]\n",
      "700 None 0.057377815 [0.72179306] [0.6324298]\n",
      "800 None 0.035285737 [0.7818297] [0.4959523]\n",
      "900 None 0.021699736 [0.8289106] [0.38892657]\n",
      "1000 None 0.01334473 [0.8658314] [0.3049967]\n",
      "1100 None 0.00820664 [0.8947848] [0.23917891]\n",
      "1200 None 0.005046841 [0.9174901] [0.18756437]\n",
      "1300 None 0.0031036653 [0.9352957] [0.14708824]\n",
      "1400 None 0.0019086679 [0.9492586] [0.11534683]\n",
      "1500 None 0.0011737797 [0.96020865] [0.09045516]\n",
      "1600 None 0.0007218362 [0.9687956] [0.07093499]\n",
      "1700 None 0.0004439084 [0.9755295] [0.05562723]\n",
      "1800 None 0.00027299262 [0.9808102] [0.04362299]\n",
      "1900 None 0.00016788131 [0.9849513] [0.03420916]\n",
      "2000 None 0.00010324144 [0.9881988] [0.02682685]\n"
     ]
    }
   ],
   "source": [
    "sess =tf.Session()\n",
    "#tf.variable을 사용할 경우 run전에 tf.global_variables_initializer()를 사용해야함\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 100 == 0:\n",
    "        print(step, sess.run(cost), sess.run(w), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b050d",
   "metadata": {},
   "source": [
    "### Full code with placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a738f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.0342264 [0.7811154] [0.20755665]\n",
      "200 0.027220666 [1.1369014] [0.6974932]\n",
      "400 0.008203988 [1.0751572] [0.87902856]\n",
      "600 0.0024725902 [1.0412605] [0.9786892]\n",
      "800 0.00074520375 [1.0226514] [1.0334018]\n",
      "1000 0.00022459036 [1.0124351] [1.0634389]\n",
      "1200 6.769014e-05 [1.0068269] [1.0799282]\n",
      "1400 2.0401305e-05 [1.0037479] [1.0889806]\n",
      "1600 6.148981e-06 [1.0020576] [1.0939503]\n",
      "1800 1.853588e-06 [1.0011296] [1.0966785]\n",
      "2000 5.5907356e-07 [1.0006204] [1.0981758]\n",
      "[6.1012774]\n",
      "[3.5997267]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "x = tf.placeholder(tf.float32, shape = [None])\n",
    "y = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "hy = w*x+b\n",
    "cost = tf.reduce_mean(tf.square(hy-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, w_val, b_val, _ = sess.run([cost, w, b, train], feed_dict={x:[1,2,3,4], y:[2.1,3.1,4.1,5.1]})\n",
    "    if step %200==0:\n",
    "        print(step,cost_val,w_val,b_val)\n",
    "        \n",
    "print(sess.run(hy, feed_dict={x: [5]}))\n",
    "print(sess.run(hy, feed_dict={x: [2.5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b7a0e",
   "metadata": {},
   "source": [
    "# 03) Linear Regression의 cost 최소화 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d759c0",
   "metadata": {},
   "source": [
    "### cost함수 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e33cffd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=[1,2,3]\n",
    "y=[1,2,3]\n",
    "w=tf.placeholder(tf.float32)\n",
    "hypothesis = w*x\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "w_val=[]\n",
    "cost_val=[]\n",
    "for i in range(-30,50):\n",
    "    feed_w = i*0.1\n",
    "    curr_cost, curr_w = sess.run([cost,w], feed_dict = {w: feed_w})\n",
    "    w_val.append(curr_w)\n",
    "    cost_val.append(curr_cost)\n",
    "    \n",
    "#show the cost function\n",
    "plt.plot(w_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107e5de",
   "metadata": {},
   "source": [
    "### cost함수 미분하기\n",
    "    위의 cost function에서 gradient descent를 활용하여 미분하기\n",
    "    cost(w)를 미분했을 때 양/음 값이 나오면 w값을 -/+조절하는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimize: Gradient Descent using derivative(미분):\n",
    "#w -= Learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((w*x-y)*x)\n",
    "descent = w - learning_rate * gradient\n",
    "update = w.assign(descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7322a276",
   "metadata": {},
   "source": [
    "## full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1e2a358",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-77-e016a020bed2>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-77-e016a020bed2>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    이 부분이\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x_data=[1,2,3]\n",
    "y_data=[1,2,3]\n",
    "\n",
    "w=tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "x=tf.placeholder(tf.float32, shape=[None])\n",
    "y=tf.placeholder(tf.float32, shape=[None])\n",
    "hy=w*x\n",
    "cost = tf.reduce_mean(tf.square(hy-y))\n",
    "\n",
    "#이 부분이\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#train = optimizer.minimize(cost)\n",
    "#로 대체할 수 있는 것\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((w*x-y)*x)\n",
    "descent = w - learning_rate * gradient\n",
    "update = w.assign(descent)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={x:x_data,y:y_data})\n",
    "    print(step, sess.run(cost,feed_dict={x:x_data,y:y_data}), sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd26a2",
   "metadata": {},
   "source": [
    "# 04-1) multi-variable linear regression 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92809e",
   "metadata": {},
   "source": [
    "### Matrix (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e70abb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 6857.9805 \n",
      "prediction:\n",
      " [230.88612 268.25198 268.94876 295.0552  200.41382] \n",
      "\n",
      "1000 cost: 19.456238 \n",
      "prediction:\n",
      " [155.80194 181.36035 181.5949  200.04129 134.87935] \n",
      "\n",
      "2000 cost: 12.462451 \n",
      "prediction:\n",
      " [154.45819 182.28967 181.19348 199.67053 136.16678] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#여기부터\n",
    "x1_data = [73,93, 89, 96, 73]\n",
    "x2_data = [80, 88, 91, 98, 66]\n",
    "x3_data = [75, 93, 90, 100, 70]\n",
    "y_data = [152, 185, 180, 196, 142]\n",
    "\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "#여기까지 매트리스로 구현 가능\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1*w1 + x2*w2 + x3*w3 +b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                        feed_dict={x1: x1_data, x2:x2_data, x3: x3_data, y: y_data})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, \"cost:\", cost_val, \"\\nprediction:\\n\", hy_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e316d",
   "metadata": {},
   "source": [
    "### Matrix (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4e91314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 12.730005 \n",
      "prediction:\n",
      " [[151.74472]\n",
      " [189.52249]\n",
      " [183.12512]\n",
      " [198.33475]\n",
      " [147.28342]] \n",
      "\n",
      "1000 cost: 2.383389 \n",
      "prediction:\n",
      " [[149.75215]\n",
      " [185.91283]\n",
      " [180.19905]\n",
      " [195.09326]\n",
      " [144.27356]] \n",
      "\n",
      "2000 cost: 1.4940068 \n",
      "prediction:\n",
      " [[150.24074]\n",
      " [185.57631]\n",
      " [180.3467 ]\n",
      " [195.21582]\n",
      " [143.81873]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[73,80,75],[93,88,93],[89,91,90],[96,98,100],[73,66,70]]\n",
    "y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,3])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "#w는 normal([들어오는 값,나가는 값])\n",
    "#b는 nomal([나가는 값])\n",
    "w = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(x,w)+b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                        feed_dict={x: x_data, y: y_data})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, \"cost:\", cost_val, \"\\nprediction:\\n\", hy_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6f5bc",
   "metadata": {},
   "source": [
    "# 04-2) 파일에서 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a567fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# Make sure the shape and data are OK\n",
    "#print(x_data.shape, \"\\n\", x_data, len(x_data))\n",
    "#print(y_data.shape, \"\\n\", y_data, len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c38358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Score will be [[185.00087]]\n",
      "Your Score will be [[172.86266]\n",
      " [178.56493]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,3])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "#w는 normal([들어오는 값,나가는 값])\n",
    "#b는 nomal([나가는 값])\n",
    "w = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(x,w)+b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                        feed_dict={x: x_data, y: y_data})\n",
    "    #if step % 1000 == 0:\n",
    "    #    print(step, \"cost:\", cost_val, \"\\nprediction:\\n\", hy_val, \"\\n\")\n",
    "\n",
    "print(\"Your Score will be\", sess.run(hypothesis, feed_dict={x: [[100,70,101]]}))\n",
    "print(\"Your Score will be\", sess.run(hypothesis, feed_dict={x: [[60,70,110],[90,100,80]]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a3783",
   "metadata": {},
   "source": [
    "### Queue Runners 이용 - 이해 부족"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a4c7b",
   "metadata": {},
   "source": [
    "    파일의 양, 크기가 너무 클 경우 용량이 부족해진다.\n",
    "    tensorflow의 queue runner를 이용하여 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-563d13f86f69>:3: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-1-563d13f86f69>:4: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-1-563d13f86f69>:11: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-563d13f86f69>:29: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    ['data-01-test-scroe.csv'], shuffle=False, name='filename_queue')\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "record_defaults = [[0.],[0.],[0.],[0.]]\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "train_x_batch, train_y_batch = \\\n",
    "    tf.train.batch([xy[0:-1], xy[-1]], batch_size=10)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,3])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "#w는 normal([들어오는 값,나가는 값])\n",
    "#b는 nomal([나가는 값])\n",
    "w = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(x,w)+b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                        feed_dict={x: x_data, y: y_data})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, \"cost:\", cost_val, \"\\nprediction:\\n\", hy_val, \"\\n\")\n",
    "        \n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1db6a8",
   "metadata": {},
   "source": [
    "# 05) Logistic Classification 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a74518d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6067502\n",
      "1000 0.48719576\n",
      "2000 0.40847698\n",
      "3000 0.34736288\n",
      "4000 0.299703\n",
      "5000 0.26227972\n",
      "6000 0.2325253\n",
      "7000 0.20851499\n",
      "8000 0.18884479\n",
      "9000 0.17249645\n",
      "10000 0.1587277\n",
      "\n",
      "Hypothesis: [[0.03471711]\n",
      " [0.16390936]\n",
      " [0.32336465]\n",
      " [0.77305526]\n",
      " [0.9342066 ]\n",
      " [0.97838426]] \n",
      "correct(y): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "#w는 normal([들어오는 값,나가는 값])\n",
    "#b는 nomal([나가는 값])\n",
    "w = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(x,w)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#if문 같은 것.\n",
    "# true if hypothesis>0.5 and return(TorF) in float. T=1, F=0\n",
    "predicted = tf.cast(hypothesis>0.5, dtype = tf.float32)\n",
    "\n",
    "#predict값과 y의 값이 같으면 true, false를 float으로 반환하여 평균을 냄\n",
    "#값이 높을수록 정확도가 높은 것\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,y),dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={x: x_data, y:y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step,cost_val)\n",
    "            \n",
    "    h,c,a = sess.run([hypothesis,predicted,accuracy], feed_dict={x: x_data, y:y_data})\n",
    "    print(\"\\nHypothesis:\",h,\"\\ncorrect(y):\",c,\"\\nAccuracy:\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc729a8",
   "metadata": {},
   "source": [
    "### Classifying diabetes (실제 데이터로 당뇨병 예측하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab7cc8e6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.78851503\n",
      "1000 0.19812554\n",
      "2000 0.13121735\n",
      "3000 0.09905819\n",
      "4000 0.079812214\n",
      "5000 0.06692143\n",
      "6000 0.057655923\n",
      "7000 0.05066346\n",
      "8000 0.04519385\n",
      "9000 0.040795926\n",
      "10000 0.037181523\n",
      "\n",
      "Hypothesis: [[0.07710411]\n",
      " [0.9445081 ]\n",
      " [0.01855678]\n",
      " [0.9809976 ]\n",
      " [0.01058839]] \n",
      "correct(y): [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter = ',', dtype=np.float32)\n",
    "x_data=xy[:,0:-1]#여기 한번 수정해보기\n",
    "y_data=xy[:,[-1]]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None,8])\n",
    "y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "#w는 normal([들어오는 값,나가는 값])\n",
    "#b는 nomal([나가는 값])\n",
    "w = tf.Variable(tf.random_normal([8,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(x,w)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#if문 같은 것.\n",
    "# true if hypothesis>0.5 and return(TorF) in float. T=1, F=0\n",
    "predicted = tf.cast(hypothesis>0.5, dtype = tf.float32)\n",
    "\n",
    "#predict값과 y의 값이 같으면 true, false를 float으로 반환하여 평균을 냄\n",
    "#값이 높을수록 정확도가 높은 것\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,y),dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={x: x_data, y:y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step,cost_val)\n",
    "            \n",
    "    h,c,a = sess.run([hypothesis,predicted,accuracy], feed_dict={x: x_data, y:y_data})\n",
    "    print(\"\\nHypothesis:\",h,\"\\ncorrect(y):\",c,\"\\nAccuracy:\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d3131",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# 06-1) Softmax Classification 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bbca9",
   "metadata": {},
   "source": [
    "### 배운 이론을 식으로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1563c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#먼저 XW=Y를 만들어주기 위해\n",
    "Z = tf.matmul(x,w)+b\n",
    "\n",
    "#hypothesis에 기존에는 sigmoid함수를 사용했다면\n",
    "hypothesis = tf.nn.softmax(Z)\n",
    "\n",
    "#cost function으로 나타내기\n",
    "cost= tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis),axis=1))\n",
    "\n",
    "#cost function 최소화하기\n",
    "optimizer = tf.train.GradienDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#arg_max\n",
    "#훈련시킨 다음, 데이터를 넣어서 hypothesis로 나온 값을 one-hot codind된 수로 출력하기\n",
    "a = sess.run(hypothesis, feed_dict={x:[넣을 데이터 값]})\n",
    "print(a, sess.run(tf.arg_max(a,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85df26",
   "metadata": {},
   "source": [
    "### 실제 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9249b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0332339e-02 9.8965961e-01 8.0269629e-06]\n",
      " [8.0925775e-01 1.7901002e-01 1.1732193e-02]\n",
      " [1.4909554e-08 3.5893073e-04 9.9964106e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = [[1,2,1,1,],[2,1,3,2,],[3,1,3,4,],[4,1,5,5,],[1,7,5,5],[1,2,5,6],[1,6,6,6,],[1,7,7,7,]]\n",
    "#one-hot encoding\n",
    "y_data = [[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]]\n",
    "\n",
    "x=tf.placeholder(\"float\",[None, 4])\n",
    "y=tf.placeholder(\"float\",[None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "#w는 normal([들어오는 값,나가는 값])\n",
    "#b는 nomal([나가는 값])\n",
    "w = tf.Variable(tf.random_normal([4, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(x,w)+b)\n",
    "\n",
    "#cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={x:x_data, y:y_data})\n",
    "    \n",
    "    a = sess.run(hypothesis, feed_dict={x: [[1,11,7,9],[1,3,4,3],[1,1,0,1]]})\n",
    "    print(a, sess.run(tf.math.argmax(a,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c4e65",
   "metadata": {},
   "source": [
    "# 06-2) Fancy한 Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146aa790",
   "metadata": {},
   "source": [
    "cross_entropy, one_hot, reshape 함수를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d8978",
   "metadata": {},
   "source": [
    "### tf.nn.softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675be7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_x = tf.matual(x,w)+b\n",
    "hypothesis = tf.nn.softmax(logits_x)\n",
    "\n",
    "#1 기존\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hpyothesis), axis=1))\n",
    "\n",
    "#2 함수 사용\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits_x,\n",
    "                                                labels = y)\n",
    "cost = tf.reduce_mean(cost_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56908c96",
   "metadata": {},
   "source": [
    "### tf.one_hot() & tf.reshape()\n",
    "one_hot으로 인해 실제 데이터에 한 차원이 더 생김. reshape으로 되돌려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 7 #y값의 범위. 해당예시) 0~6\n",
    "y = tf.placholder(tf.int32,[None,1]) #shape=(?,1)\n",
    "y_one_hot = tf.one_hot(y,nb_classes) #shape=(?,1,7)\n",
    "y_one_hot = tf.reshape(Y_one_hot, [-1,nb_classes]) #shape=(?,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20350732",
   "metadata": {},
   "source": [
    "### 실제 데이터로 classification\n",
    "TF버전이 바뀌면서 돌아가지 않음\n",
    "\n",
    "TF2 버전 코드\n",
    "\n",
    "https://github.com/hunkim/DeepLearningZeroToAll/tree/master/tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bcd5c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-106-15710b52eec7>:23: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert value <tf.Tensor 'ArgMax_4:0' shape=(?,) dtype=int64> to a TensorFlow DType.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-15710b52eec7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, dtype, name)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcast\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m   \"\"\"\n\u001b[1;32m--> 671\u001b[1;33m   \u001b[0mbase_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m   if isinstance(x,\n\u001b[0;32m    673\u001b[0m                 (ops.Tensor, _resource_variable_type)) and base_type == x.dtype:\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[1;34m(type_value)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m   raise TypeError(\"Cannot convert value %r to a TensorFlow DType.\" %\n\u001b[1;32m--> 717\u001b[1;33m                   (type_value,))\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert value <tf.Tensor 'ArgMax_4:0' shape=(?,) dtype=int64> to a TensorFlow DType."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:,:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "nb_classes = 7\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,16])\n",
    "y = tf.placeholder(tf.int32, [None,1])\n",
    "\n",
    "y_one_hot = tf.one_hot(y,nb_classes)\n",
    "y_one_hot = tf.reshape(y_one_hot, [-1, nb_classes])\n",
    "\n",
    "w = tf.Variable(tf.random_normal([16,nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'weight')\n",
    "\n",
    "logits = tf.matmul(x,w)+b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "answer = tf.equal(prediction, tf.argmax(y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(answer,prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={x:x_data, y:y_data})\n",
    "        if step %100 == 0:\n",
    "            lass, acc = sess.run([cost, accuracy], feed_dict={x:x_data, y:y_data})\n",
    "            print(\"step:{:5}\\tLoss: {:.3f}\\t.Acc: {:.2%}\".format(step,loss,acc))\n",
    "    \n",
    "    predict = sess.run(prediction, feed_data={x:x_data})\n",
    "    \n",
    "    for p,y in zip(predict,y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {} Real Y: {}\".format(p==int(y),p,int(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa953f4",
   "metadata": {},
   "source": [
    "# 07-1) training/test dataset, learning rate, normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71eace7",
   "metadata": {},
   "source": [
    "### Min_max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)\n",
    "\n",
    "'''\n",
    "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
    " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
    " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
    " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
    " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
    " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
    " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
    " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
    "'''\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "tf.model.add(tf.keras.layers.Dense(units=1, input_dim=4))\n",
    "tf.model.add(tf.keras.layers.Activation('linear'))\n",
    "tf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5))\n",
    "tf.model.summary()\n",
    "\n",
    "history = tf.model.fit(x_data, y_data, epochs=1000)\n",
    "\n",
    "predictions = tf.model.predict(x_data)\n",
    "score = tf.model.evaluate(x_data, y_data)\n",
    "\n",
    "print('Prediction: \\n', predictions)\n",
    "print('Cost: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50137b6a",
   "metadata": {},
   "source": [
    "### Learning rate and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "# try different learning_rate\n",
    "# learning_rate = 65535  # ? it works too hahaha\n",
    "learning_rate = 0.1\n",
    "# learning_rate = 1e-10  # small learning rate won't work either\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "tf.model.add(tf.keras.layers.Dense(units=3, input_dim=3, activation='softmax'))\n",
    "tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "tf.model.fit(x_data, y_data, epochs=1000)\n",
    "\n",
    "# predict\n",
    "print(\"Prediction: \", tf.model.predict_classes(x_test))\n",
    "\n",
    "# Calculate the accuracy\n",
    "print(\"Accuracy: \", tf.model.evaluate(x_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d9619",
   "metadata": {},
   "source": [
    "# 07-2) MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4376a175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_Data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_Data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_Data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_Data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 2.630485562\n",
      "Epoch: 0002 cost = 1.063533711\n",
      "Epoch: 0003 cost = 0.863300321\n",
      "Epoch: 0004 cost = 0.759450032\n",
      "Epoch: 0005 cost = 0.695155070\n",
      "Epoch: 0006 cost = 0.648511988\n",
      "Epoch: 0007 cost = 0.612543881\n",
      "Epoch: 0008 cost = 0.584208833\n",
      "Epoch: 0009 cost = 0.559770311\n",
      "Epoch: 0010 cost = 0.540906917\n",
      "Epoch: 0011 cost = 0.523227407\n",
      "Epoch: 0012 cost = 0.506941021\n",
      "Epoch: 0013 cost = 0.494610803\n",
      "Epoch: 0014 cost = 0.481564887\n",
      "Epoch: 0015 cost = 0.471822427\n",
      "Accuracy:  0.8886\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "y = tf.placeholder(tf.float32, [None,nb_classes])\n",
    "\n",
    "w = tf.Variable(tf.random_normal([784,nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_Data/\", one_hot = True)\n",
    "batch_cs, bath_ys = mnist.train.next_batch(100)\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(x,w)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis), axis = 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.math.argmax(hypothesis, 1), tf.math.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670e6e6",
   "metadata": {},
   "source": [
    "# 08) Tensor Manipluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692365d",
   "metadata": {},
   "source": [
    "#### simple ID array and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b0120dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0., 1., 2., 3., 4., 5., 6.])\n",
      "1\n",
      "(7,)\n",
      "0.0 1.0 2.0\n",
      "[0. 1. 2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "\n",
    "t = np.array([0.,1.,2.,3.,4.,5.,6.])\n",
    "pp.pprint(t)\n",
    "print(t.ndim) #rank #1차원\n",
    "print(t.shape) #shape #모양, 몇 개가 있는지\n",
    "print(t[0], t[1], t[2])\n",
    "print(t[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "178c3b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.,  2.,  3.],\n",
      "       [ 4.,  5.,  6.],\n",
      "       [ 7.,  8.,  9.],\n",
      "       [10., 11., 12.]])\n",
      "2\n",
      "(4, 3)\n",
      "[1. 2. 3.] [4. 5. 6.] [7. 8. 9.]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "\n",
    "t = np.array([[1.,2.,3.],[4.,5.,6.],[7.,8.,9.],[10.,11.,12.]])\n",
    "pp.pprint(t)\n",
    "print(t.ndim) #rank:2차원, 첫 '[' 갯수\n",
    "print(t.shape) #shape: 뒤에서부터 가장 안쪽 차원에 있는 원소의 갯수\n",
    "print(t[0], t[1], t[2])\n",
    "print(t[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a95cf",
   "metadata": {},
   "source": [
    "#### Shape, Rank, Axis\n",
    "    rank: 2 -> shape:[?,?]\n",
    "    rank: 3 -> shape:[?,?,?]\n",
    "    Axis는 매트리스를 표현했을 때, y축을 의미. 왼쪽부터 0\n",
    "    위에서는 axis = 0은 [1,4,7,10]부분.\n",
    "    axis = 1은 [1,2,3], [4,5,6], [7,8,9], [10,11,12]를 의미.\n",
    "    아직 axis개념 부족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23658a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "\n",
    "t = tf.constant([1,2,3,4])\n",
    "tf.shape(t).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75784f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1,2],[3,4]])\n",
    "tf.shape(t).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6ca5e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[[[1,2,3,4],[5,6,7,8,],[9,10,11,12]],\n",
    "                  [[13,14,15,16],[17,18,19,20],[21,22,23,24]]]])\n",
    "tf.shape(t).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958a828",
   "metadata": {},
   "source": [
    "#### Matmul VS multiply\n",
    "matmul은 매트릭스의 곱이고, 단순 multiply는 산수곱이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdc84e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1 shape:  (2, 2)\n",
      "matrix2 shape:  (2, 1)\n",
      "[[ 5]\n",
      " [11]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "\n",
    "matrix1 = tf.constant([[1,2],[3,4]])\n",
    "matrix2 = tf.constant([[1],[2]])\n",
    "print(\"matrix1 shape: \", matrix1.shape)\n",
    "print(\"matrix2 shape: \", matrix2.shape)\n",
    "print(tf.matmul(matrix1,matrix2).eval(session=sess))\n",
    "print(tf.matmul(matrix1,matrix2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dce2b2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [6, 8]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matrix1*matrix2).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12898d6",
   "metadata": {},
   "source": [
    "#### Reduce mean / Reduce_sum\n",
    "int가 아닌 float으로 계산해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76dd23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "\n",
    "print(tf.reduce_mean([1,2], axis = 0).eval(session=sess))\n",
    "print(tf.reduce_sum([1,2], axis = 0).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0dec8f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "x = [[1.,2.],[3.,4.]]\n",
    "\n",
    "print(tf.reduce_mean(x).eval(session=sess))\n",
    "print(tf.reduce_sum(x).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81a6001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n",
      "[4. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(x, axis = 0).eval(session=sess))\n",
    "print(tf.reduce_sum(x, axis = 0).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7c6f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5 3.5]\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(x, axis = 1).eval(session=sess))\n",
    "print(tf.reduce_sum(x, axis = 1).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42bbd1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5 3.5]\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(x, axis = -1).eval(session=sess))\n",
    "print(tf.reduce_sum(x, axis = -1).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db41ff6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.reduce_sum(x, axis = -1)).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c36fa",
   "metadata": {},
   "source": [
    "#### Argmax\n",
    "가장 큰 값의 위치 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "047fe59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[0,1,2],\n",
    "     [2,1,0]]\n",
    "tf.argmax(x, axis = 0).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18a4f0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(x, axis = 1).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02293350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(x, axis = -1).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0c882",
   "metadata": {},
   "source": [
    "#### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea4c86ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([[[0,1,2],\n",
    "               [3,4,5]],\n",
    "              \n",
    "              [[6,7,8],\n",
    "               [9,10,11]]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a43ca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(t, shape=[-1,3]).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1113a842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2]],\n",
       "\n",
       "       [[ 3,  4,  5]],\n",
       "\n",
       "       [[ 6,  7,  8]],\n",
       "\n",
       "       [[ 9, 10, 11]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(t, shape=[-1,1,3]).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575edeb4",
   "metadata": {},
   "source": [
    "Reshape (squeeze, expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25146bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "\n",
    "tf.squeeze([[0],[1],[2]]).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54c42f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims([0,1,2],1).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ce70b",
   "metadata": {},
   "source": [
    "#### one hot\n",
    "one_hot을 사용하면 rank가 하나 더 추가되므로, reshape으로 바꾸어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e2cfb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "\n",
    "t = tf.one_hot([[0],[1],[2],[1],[3]], depth = 4).eval(session=sess)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c960dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(t, shape=[-1,4]).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4146a",
   "metadata": {},
   "source": [
    "#### Casting\n",
    "정수로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f475b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast([1.8,2.2,3.3,4.9], tf.int32).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a871e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast([True, False, 1 == 1, 0 == 1], tf.int32).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133cf893",
   "metadata": {},
   "source": [
    "#### Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3317c14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 7],\n",
       "       [2, 5, 8],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,4,7]\n",
    "y = [2,5,8]\n",
    "z = [3,6,9]\n",
    "\n",
    "tf.stack([x,y,z]).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d119cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 7],\n",
       "       [2, 5, 8],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([x,y,z], axis = 0).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9b67fcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([x,y,z], axis = 1).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a898d491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([x,y,z], axis = -1).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044af0f",
   "metadata": {},
   "source": [
    "#### Ones and Zeros like\n",
    "똑같은 모양으로 1 or 0으로 채워진 tens를 만들고 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6ce7aa38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =[[0,1,2],\n",
    "    [2,1,0]]\n",
    "tf.ones_like(x).eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6be8270b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros_like(x).eval(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38b513",
   "metadata": {},
   "source": [
    "#### Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f35bd0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip([1,2,3],[4,5,6]):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e2ac0cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "for x,y,z in zip([1,2,3],[4,5,6],[7,8,9]):\n",
    "    print(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf50db",
   "metadata": {},
   "source": [
    "# 09-1) Neural Net for XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a9c1d",
   "metadata": {},
   "source": [
    "아래처럼 구하면 결과가 나오지 않음. 그 이유는 레이어가 하나라서. 그러므로 레이어를 더 추가해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6f85e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.80094564 [[0.50466466]\n",
      " [1.559028  ]]\n",
      "2000 0.69314706 [[0.00046584]\n",
      " [0.00046973]]\n",
      "4000 0.6931472 [[1.6850956e-07]\n",
      " [1.6900614e-07]]\n",
      "6000 0.6931472 [[1.3274664e-07]\n",
      " [1.3324322e-07]]\n",
      "8000 0.6931472 [[1.3274664e-07]\n",
      " [1.3324322e-07]]\n",
      "10000 0.6931472 [[1.3274664e-07]\n",
      " [1.3324322e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "#XOR with logistic regression\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "w = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "#hypothesis using sigmoid\n",
    "hypothesis = tf.sigmoid(tf.matmul(x,w)+b)\n",
    "\n",
    "#cost/loss function\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "                                                                     \n",
    "#Accuracy computation\n",
    "#True if hypothesis >0.5 else False\n",
    "predicted = tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,y), dtype = tf.float32))\n",
    "\n",
    "#Launch graph\n",
    "with tf.Session() as sess:\n",
    "    #initialize tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: x_data, y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y:y_data}), sess.run(w))\n",
    "            \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e12efd",
   "metadata": {},
   "source": [
    "레이어 추가해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ed71a1f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7064974 [array([[-0.8171747 ,  0.57096416],\n",
      "       [ 0.08401517, -1.1195091 ]], dtype=float32), array([[-0.27196887],\n",
      "       [ 1.2272326 ]], dtype=float32)]\n",
      "2000 0.67902285 [array([[-0.9415418 , -0.98512304],\n",
      "       [-0.43661335, -1.017343  ]], dtype=float32), array([[-0.5640654],\n",
      "       [ 1.3629353]], dtype=float32)]\n",
      "4000 0.18275967 [array([[-4.1249113, -3.3094745],\n",
      "       [-4.1097035, -3.3031285]], dtype=float32), array([[-5.7477336],\n",
      "       [ 4.9115906]], dtype=float32)]\n",
      "6000 0.04809258 [array([[-5.3413525, -4.4068675],\n",
      "       [-5.3367634, -4.4055114]], dtype=float32), array([[-8.360147],\n",
      "       [ 7.608292]], dtype=float32)]\n",
      "8000 0.025550209 [array([[-5.7786655, -4.834289 ],\n",
      "       [-5.7757406, -4.833503 ]], dtype=float32), array([[-9.505008],\n",
      "       [ 8.820984]], dtype=float32)]\n",
      "10000 0.017100647 [array([[-6.0319076, -5.085617 ],\n",
      "       [-6.029661 , -5.085038 ]], dtype=float32), array([[-10.227912],\n",
      "       [  9.581648]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.01403944]\n",
      " [0.9846677 ]\n",
      " [0.984671  ]\n",
      " [0.02309409]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "#XOR with logistic regression\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,2]), name = 'weight1')\n",
    "# x를 두개로 나누었으니 input:2, output: 임의로 2라 설정\n",
    "# Wide NN for XOR: 만약 output을 10까지 넓히면 더 정확한 값이 나온다.\n",
    "# Deep NN for XOR: w1,2뿐만 아니라 wN까지 깊게 연결한다면 더 정확한 값이 나온다.\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "#output이 2이므로 여기도 2\n",
    "\n",
    "layer1 = tf.sigmoid(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name = 'weight2') #layer1의 값이 2개이니 input:2, output: y이므로 1\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "\n",
    "#hypothesis using sigmoid\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,w2)+b2)\n",
    "\n",
    "#cost/loss function\n",
    "cost = -tf.reduce_mean(y*tf.log(hypothesis)+(1-y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "                                                                     \n",
    "#Accuracy computation\n",
    "#True if hypothesis >0.5 else False\n",
    "predicted = tf.cast(hypothesis>0.5,dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,y), dtype = tf.float32))\n",
    "\n",
    "#Launch graph\n",
    "with tf.Session() as sess:\n",
    "    #initialize tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: x_data, y:y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y:y_data}), sess.run([w1, w2]))\n",
    "            \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2217f8",
   "metadata": {},
   "source": [
    "# 09-2) Tensorboard (Neural Net for XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9298a3",
   "metadata": {},
   "source": [
    "TensorBoard: TF logging/debugging tool\n",
    "\n",
    "- visualize TF graph\n",
    "- plot quantitative metrics\n",
    "- show additional data\n",
    "\n",
    "#### 5 Steps of using TensorBoard\n",
    "\n",
    "1) From TF graph, decide which tensors you want to log\n",
    "    \n",
    "   - w2_hist = tf.summary.histogram(\"weights2\",w2)\n",
    "   - cost_summ = tf.summary.scalar(\"cost\",cost)\n",
    "\n",
    "2) Merge all summaries\n",
    "   \n",
    "   - summary = tf.summary.merge_all()\n",
    "\n",
    "3) Create writer and add graph\n",
    "    \n",
    "   - writer = tf.summary.FileWriter('./logs') <=파일의 위치\n",
    "\n",
    "4) Run summary merge and add_summary\n",
    "    \n",
    "   - s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "   - writer.add_summary(s, global_step=global_step)\n",
    "\n",
    "5) Launch TensorBoard\n",
    "    \n",
    "   - tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e5606",
   "metadata": {},
   "source": [
    "# 10) NN, ReLu, Xavier, Dropout and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0fdce",
   "metadata": {},
   "source": [
    "#### NN for MNIST using ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7da66",
   "metadata": {},
   "source": [
    "아래 코드는 왜 accuracy가 낮은가..?\n",
    "\n",
    "import random / tf.set_random_seed(777)을 추가해줬더니\n",
    "이번엔 아래 에러가 뜨네,,^^\n",
    "\n",
    "InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_96' with dtype float and shape [?,784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4e7a04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_Data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_Data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_Data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_Data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 38.762562066\n",
      "Epoch: 0002 cost = 2.335852348\n",
      "Epoch: 0003 cost = 2.273596875\n",
      "Epoch: 0004 cost = 2.235497223\n",
      "Epoch: 0005 cost = 2.213783683\n",
      "Epoch: 0006 cost = 2.209784639\n",
      "Epoch: 0007 cost = 2.185846596\n",
      "Epoch: 0008 cost = 2.168562323\n",
      "Epoch: 0009 cost = 2.147011697\n",
      "Epoch: 0010 cost = 2.117015630\n",
      "Epoch: 0011 cost = 2.100451626\n",
      "Epoch: 0012 cost = 2.077782159\n",
      "Epoch: 0013 cost = 2.066289157\n",
      "Epoch: 0014 cost = 2.051630605\n",
      "Epoch: 0015 cost = 2.052933160\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_98' with dtype float and shape [?,784]\n\t [[node Placeholder_98 (defined at <ipython-input-54-34f831b7580f>:16) ]]\n\nOriginal stack trace for 'Placeholder_98':\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-54-34f831b7580f>\", line 16, in <module>\n    x = tf.placeholder(tf.float32, [None, 784])\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7400, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_98' with dtype float and shape [?,784]\n\t [[{{node Placeholder_98}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-34f831b7580f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     print('Accuracy:', sess.run(accuracy, feed_dict={\n\u001b[1;32m---> 57\u001b[1;33m           X: mnist.test.images, Y: mnist.test.labels}))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_98' with dtype float and shape [?,784]\n\t [[node Placeholder_98 (defined at <ipython-input-54-34f831b7580f>:16) ]]\n\nOriginal stack trace for 'Placeholder_98':\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3166, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-54-34f831b7580f>\", line 16, in <module>\n    x = tf.placeholder(tf.float32, [None, 784])\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7400, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\jiho kang\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_Data/\", one_hot = True)\n",
    "#batch_cs, bath_ys = mnist.train.next_batch(100)\n",
    "\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([784,256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([256,256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([256,10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.math.argmax(hypothesis, 1), tf.math.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    #print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fde5e",
   "metadata": {},
   "source": [
    "모두의 딥러닝 깃허브 코드 - NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0e4ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 207.040254787\n",
      "Epoch: 0002 cost = 43.420535611\n",
      "Epoch: 0003 cost = 26.916869218\n",
      "Epoch: 0004 cost = 18.656326479\n",
      "Epoch: 0005 cost = 13.558684814\n",
      "Epoch: 0006 cost = 10.001984177\n",
      "Epoch: 0007 cost = 7.499845996\n",
      "Epoch: 0008 cost = 5.539930624\n",
      "Epoch: 0009 cost = 4.150094018\n",
      "Epoch: 0010 cost = 3.041470631\n",
      "Epoch: 0011 cost = 2.264488540\n",
      "Epoch: 0012 cost = 1.649692378\n",
      "Epoch: 0013 cost = 1.273051429\n",
      "Epoch: 0014 cost = 0.937638706\n",
      "Epoch: 0015 cost = 0.715348533\n",
      "Learning Finished!\n",
      "Accuracy: 0.941\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8b9ad",
   "metadata": {},
   "source": [
    "#### Xavier for MNIST\n",
    "초기값 알맞게 initialize 해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa859271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_Data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_Data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_Data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_Data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 3.337526592\n",
      "Epoch: 0002 cost = 2.086277740\n",
      "Epoch: 0003 cost = 2.078587989\n",
      "Epoch: 0004 cost = 2.081420163\n",
      "Epoch: 0005 cost = 2.089653796\n",
      "Epoch: 0006 cost = 2.084225474\n",
      "Epoch: 0007 cost = 2.100225656\n",
      "Epoch: 0008 cost = 2.088301923\n",
      "Epoch: 0009 cost = 2.086568934\n",
      "Epoch: 0010 cost = 2.084616396\n",
      "Epoch: 0011 cost = 2.080519520\n",
      "Epoch: 0012 cost = 2.079773072\n",
      "Epoch: 0013 cost = 2.137010396\n",
      "Epoch: 0014 cost = 2.090988021\n",
      "Epoch: 0015 cost = 2.125597871\n",
      "Accuracy:  0.151\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_Data/\", one_hot = True)\n",
    "batch_cs, bath_ys = mnist.train.next_batch(100)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w1 = tf.get_variable(\"w1\", shape=[784,256],\n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2 = tf.get_variable(\"w2\", shape=[256,256],\n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3 = tf.get_variable(\"w3\", shape=[256,10],\n",
    "                     initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.math.argmax(hypothesis, 1), tf.math.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost,optimizer], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d20adb",
   "metadata": {},
   "source": [
    "모두의 딥러닝 깃허브 코드 - xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d48f9ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.305613608\n",
      "Epoch: 0002 cost = 0.114584134\n",
      "Epoch: 0003 cost = 0.074886081\n",
      "Epoch: 0004 cost = 0.053719383\n",
      "Epoch: 0005 cost = 0.038663745\n",
      "Epoch: 0006 cost = 0.030411467\n",
      "Epoch: 0007 cost = 0.024180686\n",
      "Epoch: 0008 cost = 0.018463352\n",
      "Epoch: 0009 cost = 0.016614096\n",
      "Epoch: 0010 cost = 0.015713875\n",
      "Epoch: 0011 cost = 0.013593401\n",
      "Epoch: 0012 cost = 0.010412591\n",
      "Epoch: 0013 cost = 0.010473258\n",
      "Epoch: 0014 cost = 0.011443031\n",
      "Epoch: 0015 cost = 0.009695913\n",
      "Learning Finished!\n",
      "Accuracy: 0.9795\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba0bf0",
   "metadata": {},
   "source": [
    "#### Deep NN for MNIST\n",
    "너무 깊어지고 넓어지면 overfitting이 될 수 있다.\n",
    "\n",
    "모두의 딥러닝 깃허브 코드 - Deep NN\n",
    "\n",
    "대체 뭐가 문제인가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e61be674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"W1:0\", shape=(784, 512), dtype=float32_ref) must be from the same graph as Tensor(\"Placeholder:0\", shape=(?, 784), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-bf467af0c167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                      initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0;32m     24\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2567\u001b[0m       \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2568\u001b[0m   \"\"\"\n\u001b[1;32m-> 2569\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2571\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only one of transpose_a and adjoint_a can be True.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6506\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6507\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6508\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6509\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6510\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   6133\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6134\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6135\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6136\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6137\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   6069\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6070\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[1;32m-> 6071\u001b[1;33m                      (item, original_item))\n\u001b[0m\u001b[0;32m   6072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"W1:0\", shape=(784, 512), dtype=float32_ref) must be from the same graph as Tensor(\"Placeholder:0\", shape=(?, 784), dtype=float32)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538efa1",
   "metadata": {},
   "source": [
    "#### Dropout for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e01c1dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"W1:0\", shape=(784, 512), dtype=float32_ref) must be from the same graph as Tensor(\"Placeholder_1:0\", shape=(?, 784), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-8c6842801b68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                      initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0;32m     27\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2567\u001b[0m       \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2568\u001b[0m   \"\"\"\n\u001b[1;32m-> 2569\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2571\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only one of transpose_a and adjoint_a can be True.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6506\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6507\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6508\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6509\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6510\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   6133\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6134\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6135\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6136\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6137\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   6069\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6070\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" %\n\u001b[1;32m-> 6071\u001b[1;33m                      (item, original_item))\n\u001b[0m\u001b[0;32m   6072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"W1:0\", shape=(784, 512), dtype=float32_ref) must be from the same graph as Tensor(\"Placeholder_1:0\", shape=(?, 784), dtype=float32)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395ee48",
   "metadata": {},
   "source": [
    "# 11-1) CNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7267f",
   "metadata": {},
   "source": [
    "#### Simple convolution layer\n",
    "\n",
    "    input: 3x3x1\n",
    "    filter: 2x2x1\n",
    "    output: 2x2x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a2ae9",
   "metadata": {},
   "source": [
    "Toy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c2bc429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24f0a950888>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sess = tf.Session()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                    [[4],[5],[6]],\n",
    "                    [[7],[8],[9]]]], dtype = np.float32)\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(3,3), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87841df",
   "metadata": {},
   "source": [
    "    Image: 1,3,3,1\n",
    "    Filter: 2,2,1,1, ( 2x2x1 1개)\n",
    "    Stride: 1x1\n",
    "    Padding: VALID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bcb72a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_imag.shape (1, 2, 2, 1)\n",
      "[[12. 16.]\n",
      " [24. 28.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"image.shape\",image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding =\"VALID\")\n",
    "#stride = 1x1일 때, padding값을 'same'을 주면\n",
    "#input 크기와 output 크기를 갖게 맞추어 padding값을 자동으로 주게 된다.\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_imag.shape\", conv2d_img.shape)\n",
    "\n",
    "conv2d_img = np.swapaxes(conv2d_img,0,3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(2,2))\n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087afd3",
   "metadata": {},
   "source": [
    "Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65730dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgae = np.array([[[[4],[3],\n",
    "                    [2],[1]]]], dtype=np.float32)\n",
    "pool = tf.nn.max_pool(image, ksize=[1,2,2,1],#weight\n",
    "                      strides = [1,1,1,1], padding='SAME')\n",
    "print(pool.shape)\n",
    "print(pool.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1096eb",
   "metadata": {},
   "source": [
    "실전 이미지에 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f7759c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Tensor(\"Conv2D_11:0\", shape=(1, 14, 14, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "img = mnist.train.images[0].reshape(28,28)#가장 첫번째 데이터를 28,28로 출력하기\n",
    "plt.imshow(img,cmap='gray')\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "#28x28x1 사이즈로 reshape해라, 갯수는 너가 알아서 해! (-1)\n",
    "img = img.reshape(-1,28,28,1)\n",
    "\n",
    "#3x3x1 filter로 wieght를 줄 것이며, 5개 filter 사용.\n",
    "w1 = tf.Variable(tf.random_normal([3,3,1,5], stddev=0.01))\n",
    "\n",
    "#stride 2x2에서 padding='same'을 주면, input의 절반 사이즈 (여기서는 14)가 된다\n",
    "conv2d=tf.nn.conv2d(img,w1,strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "print(conv2d)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d_img=conv2d.eval()\n",
    "conv2d_img= np.swapaxes(conv2d_img,0,3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    plt.subplot(1,5,i+1),plt.imshow(one_img.reshape(14,14),cmap='gray')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e05caae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_9:0\", shape=(1, 7, 7, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Max pooling을 해보자\n",
    "pool = tf.nn.max_pool(conv2d, ksize=[1,2,2,1], strides=[\n",
    "    1,2,2,1], padding='SAME')\n",
    "print(pool)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pool_img = pool.eval()\n",
    "pool_img = np.swapaxes(pool_img,0,3)\n",
    "for i, one_img in enumerate(pool_img):\n",
    "    plt.subplot(1,5,i+1),plt.imshow(one_img.reshape(7,7),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90a6f1",
   "metadata": {},
   "source": [
    "# 11-2) MNIST 99% with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9432bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.391705905\n",
      "Epoch: 0002 cost = 0.100289708\n",
      "Epoch: 0003 cost = 0.076496792\n",
      "Epoch: 0004 cost = 0.061320314\n",
      "Epoch: 0005 cost = 0.052941998\n",
      "Epoch: 0006 cost = 0.047001821\n",
      "Epoch: 0007 cost = 0.040163770\n",
      "Epoch: 0008 cost = 0.035363829\n",
      "Epoch: 0009 cost = 0.031678290\n",
      "Epoch: 0010 cost = 0.029407677\n",
      "Epoch: 0011 cost = 0.025139195\n",
      "Epoch: 0012 cost = 0.023003524\n",
      "Epoch: 0013 cost = 0.020343913\n",
      "Epoch: 0014 cost = 0.017449826\n",
      "Epoch: 0015 cost = 0.015580827\n",
      "Learning Finished!\n",
      "Accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Conv layer1\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "#Conv layer2\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
    "'''\n",
    "\n",
    "# Fully Connected(FC,Dense) layer\n",
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cab942",
   "metadata": {},
   "source": [
    "# 11-3) CNN class, Layers, Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f29a79",
   "metadata": {},
   "source": [
    "강좌로 들어~~~~\n",
    "\n",
    "https://www.youtube.com/watch?v=c62uTWdhhMw&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6186c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
